---
apiVersion: v1
kind: Namespace
metadata:
  name: llama2

---
apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: llama2-service
  namespace: llama2
spec:
  serviceUnhealthySecondThreshold: 900 # Config for the health check threshold for Ray Serve applications. Default value is 900.
  deploymentUnhealthySecondThreshold: 300 # Config for the health check threshold for Ray dashboard agent. Default value is 300.
#  serveConfig:
#    importPath: ????
#    runtimeEnv: |
#      env_vars: {"MODEL_ID": "????"}
  rayClusterConfig:
    rayVersion: '2.7.1' # Should match the Ray version in the image of the containers
    headGroupSpec:
      serviceType: ClusterIP  # Options are ClusterIP, NodePort, and LoadBalancer
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.7.1 # Replace with Neuron docker image
              lifecycle:
                preStop:
                  exec:
                    command: [ "/bin/sh","-c","ray stop" ]
              ports:
                - containerPort: 6379
                  name: gcs
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
              resources:
                limits:
                  cpu: 2
                  memory: 8Gi
                requests:
                  cpu: 2
                  memory: 8Gi

          nodeSelector:
            provisioner: default
            workload: rayhead
          volumes:
            - name: ray-logs
              emptyDir: {}
    workerGroupSpecs:
      - groupName: inf2-worker-group
        replicas: 1
        minReplicas: 1
        maxReplicas: 1
        rayStartParams: {}
        template:
          spec:
            containers:
              - name: ray-worker
                image: rayproject/ray-ml:2.7.1  # replace with llama2 image
                lifecycle:
                  preStop:
                    exec:
                      command: [ "/bin/sh","-c","ray stop" ]
                resources:
                  limits:
                    cpu: "90"
                    memory: "300G"
                    aws.amazon.com/neuron: "6"
                  requests:
                    cpu: "90"
                    memory: "300G"
                    aws.amazon.com/neuron: "6"
            nodeSelector:
              karpenter.sh/provisioner-name: inferentia-inf2
              hub.jupyter.org/node-purpose: user
            tolerations:
              - key: aws.amazon.com/neuroncore
                operator: Exists
                effect: NoSchedule
              - key: aws.amazon.com/neuron
                operator: Exists
                effect: NoSchedule
              - key: "hub.jupyter.org/dedicated"
                operator: "Equal"
                value: "user"
                effect: "NoSchedule"

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama2
  namespace: llama2
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          # Ray Dashboard
          - path: /llama2/(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: llama2-head-svc
                port:
                  number: 8265
