---
sidebar_label: Llama 3 Fine-tuning with LoRA
---
import CollapsibleContent from '../../../../src/components/CollapsibleContent';

:::warning
To deploy this example for fine-tuning a LLM on EKS, you need access to AWS Trainium ec2 instance. If deployment fails, check if you have access to this instance type. If nodes aren't starting, check Karpenter or Node group logs.
:::

:::danger
Note: The Llama 3 model is governed by the Meta license. To download the model weights and tokenizer, visit the [website](https://ai.meta.com/) and accept the license before requesting access.
:::

:::info
We are working to improve this blueprint with better observability, logging, and scalability.
:::

# Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron

This guide shows you how to fine-tune the `Llama3-8B` language model using AWS Trainium (Trn1) EC2 instances. We'll use HuggingFace Optimum Neuron to make integration with Neuron easy.

### What is Llama 3?

Llama 3 is a large language model (LLM) for tasks like text generation, summarization, translation, and question answering. You can fine-tune it for your specific needs.

### AWS Trainium

AWS Trainium (Trn1) instances are designed for high-throughput, low-latency deep learning, ideal for training large models like Llama 3. The AWS Neuron SDK enhances Trainium's performance by optimizing models with advanced compiler techniques and mixed precision training for faster, accurate results.

## 1. Deploying the Solution

<CollapsibleContent header={<h2><span>Prerequisites</span></h2>}>
Before you start, make sure you have everything you need:
- You can use your local Mac/Windows computer or an Amazon EC2 instance.
- Install Docker (with at least 100GB free space) and make sure your Docker image uses x86 architecture.
- Install these tools and make sure your AWS user or role has the right permissions:
  * [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
  * [kubectl](https://Kubernetes.io/docs/tasks/tools/)
  * [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
  * [envsubst](https://pypi.org/project/envsubst/)
  * Git (only for EC2 instance)
  * Python, pip, jq, unzip

**Clone the ai-on-eks repository:**

Once you have the prerequisites, clone the ai-on-eks GitHub repository:

```bash
git clone https://github.com/awslabs/ai-on-eks.git
```

**Go to the trainium-inferentia directory:**

```bash
cd ai-on-eks/infra/trainium-inferentia
```

In the `terraform` sub-folder, set your preferred AWS region in the `blueprint.tfvars` file.

**Note:** Trainium instances are only available in certain regions. You can check which regions support them [here](https://repost.aws/articles/ARmXIF-XS3RO27p0Pd1dVZXQ/what-regions-have-aws-inferentia-and-trainium-instances).

Ensure provisioning of AWS FSx for Lustre CSI driver and FSx-L volume by setting `enable_aws_fsx_csi_driver` and `deploy_fsx_volume` variables in the file to `true`. The rest of the resources can be set to `false`, since they aren't used in this fine-tuning example.

Run the installation script to set up an EKS cluster with all required add-ons:

```bash
./install.sh
```

### Verify the resources

Verify the EKS cluster is running by using the region selected earlier:

```bash
aws eks --region AWS_REGION describe-cluster --name trainium-inferentia
```

Update the Kubernetes config file to authenticate with EKS using the same region:

```bash
aws eks --region AWS_REGION update-kubeconfig --name trainium-inferentia

# check the EKS Managed Node group nodes
kubectl get nodes
```

**Note:** Replace AWS_REGION with the AWS region you chose previously.


</CollapsibleContent>

## 2. Launch the Llama training job

Before we launch the training script, let's first deploy a utility pod. You will get on an interactive shell of this pod to monitor the progress of the fine-tuning job, access the fined tuned model weights, and view the output generated by the fine-tuned model for sample prompts.

```bash
kubectl apply -f training-artifact-access-pod.yaml
```

Create the ConfigMap for the training script:

```bash
kubectl apply -f llama3-finetuning-script-configmap.yaml
```

In order for the training script to be able to download the Llama 3 model from HuggingFace, it needs your HuggingFace Hub access token for authentication and accessing the model. For guidance on how to create and manage your HuggingFace tokens, please visit [Hugging Face Token Management](https://huggingface.co/docs/hub/security-tokens).

We'll set your HuggingFace Hub token as an environment variable. Replace `your_huggingface_hub_access_token` with your actual HuggingFace Hub access token.

```bash
export HUGGINGFACE_HUB_ACCESS_TOKEN=$(echo -n "your_huggingface_hub_access_token" | base64)
```

Deploy the Secret and fine-tuning Job resources by running the following command, which automatically substitutes HUGGINGFACE_HUB_ACCESS_TOKEN environment variable in the yaml before applying the resources to your Kubernetes cluster.

**Note:** The fine-tuning container image is fetched from the `us-west-2` ECR repository. Review the [HuggingFace website](https://huggingface.co/docs/optimum-neuron/en/containers) for additional guidance to check if it provides another region that you prefer based on the region you selected above for running this particular fine-tuning example. If you choose a different supported region, update the AWS account ID and region in the container image URL in the lora-finetune-resources.yaml file before running the below command.

```bash
envsubst < lora-finetune-resources.yaml | kubectl apply -f -
```

## 3. Verify fine-tuning

Check the job status:

```bash
kubectl get jobs
```

**Note:** If the container isn't scheduled, check Karpenter logs for errors. This might happen if the chosen availability zones (AZs) or subnets lack an available trn1.32xlarge EC2 instance. To fix this, update the local.azs field in the main.tf file, located at ai-on-eks/infra/base/terraform. Ensure the trainium-trn1 EC2NodeClass in the addons.tf file, also at ai-on-eks/infra/base/terraform, references the correct subnets for these AZs. Then, rerun install.sh from ai-on-eks/infra/trainium-inferentia to apply the changes via Terraform.

To monitor the log for the fine-tuning job, access the tuned model, or check the generated text-to-SQL outputs from the test run with the fine-tuned model, open a shell in the utility pod and navigate to the `/shared` folder where these can be found. The fine-tuned model will be saved in a folder named `llama3_tuned_model_<timestamp>` and the generated SQL queries from sample prompts can be found in a log file named `llama3_finetuning.out` alongside the model folder.

```bash
kubectl exec -it training-artifact-access-pod -- /bin/bash

cd /shared

ls -l llama3_tuned_model* llama3_finetuning*
```

## 4. Cleaning up

**Note:** Always run the cleanup steps to avoid extra AWS costs.

To remove the resources created by this solution, run these commands from the root of the ai-on-eks repository:

```bash
# Delete the Kubernetes resources:
cd blueprints/training/llama-lora-finetuning-trn1
envsubst < lora-finetune-resources.yaml | kubectl delete -f -
kubectl delete -f llama3-finetuning-script-configmap.yaml
kubectl delete -f training-artifact-access-pod.yaml
```

Clean up the EKS cluster and related resources:

```bash
cd ../../../infra/trainium-inferentia/terraform
./cleanup.sh
```
