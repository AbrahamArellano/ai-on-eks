---
sidebar_label: Llama 3 Fine-tuning with LoRA
---
import CollapsibleContent from '../../../../src/components/CollapsibleContent';

:::warning
To deploy ML models on EKS, you need access to GPUs or Neuron instances. If deployment fails, check if you have access to these resources. If nodes aren't starting, check Karpenter or Node group logs.
:::

:::danger
Note: The Llama 3 model is governed by the Meta license. To download the model weights and tokenizer, visit the [website](https://ai.meta.com/) and accept the license before requesting access.
:::

:::info
We are working to improve this blueprint with better observability, logging, and scalability.
:::

# Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron

This guide shows you how to fine-tune the `Llama3-8B` language model using AWS Trainium (Trn1) EC2 instances. We'll use HuggingFace Optimum Neuron to make integration with Neuron easy.

### What is Llama 3?

Llama 3 is a large language model (LLM) for tasks like text generation, summarization, translation, and question answering. You can fine-tune it for your specific needs.

#### AWS Trainium:
- **Optimized for Deep Learning**: AWS Trainium Trn1 instances are built for deep learning. They offer high throughput and low latency, making them great for training large models like Llama 3.
- **Neuron SDK**: The AWS Neuron SDK helps optimize your models for Trainium. It includes advanced compiler optimizations and supports mixed precision training for faster results without losing accuracy.

## 1. Deploying the Solution

<CollapsibleContent header={<h2><span>Prerequisites</span></h2>}>
Before you start, make sure you have everything you need:
- You can use your local Mac/Windows computer or an Amazon EC2 instance.
- Install Docker (with at least 100GB free space) and make sure your Docker image uses x86 architecture.
- Install these tools and make sure your AWS user or role has the right permissions:
  * [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
  * [kubectl](https://Kubernetes.io/docs/tasks/tools/)
  * [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
  * [envsubst](https://pypi.org/project/envsubst/)
  * Git (only for EC2 instance)
  * Python, pip, jq, unzip

**Clone the ai-on-eks repository:**

Once you have the prerequisites, clone the ai-on-eks GitHub repository:

```bash
git clone https://github.com/awslabs/ai-on-eks.git
```

**Go to the trainium-inferentia directory:**

```bash
cd ai-on-eks/infra/trainium-inferentia
```

In the `terraform` sub-folder, set your preferred AWS region in the `blueprint.tfvars` file.

**Note:** Trainium instances are only available in certain regions. You can check which regions support them [here](https://repost.aws/articles/ARmXIF-XS3RO27p0Pd1dVZXQ/what-regions-have-aws-inferentia-and-trainium-instances).

Ensure provisioning of AWS FSx for Lustre CSI driver and FSx-L volume by setting `enable_aws_fsx_csi_driver` and `deploy_fsx_volume` variables in the file to `true`. The rest of the resources can be set to `false`, since they aren't used in this fine-tuning example.

Run the installation script to set up an EKS cluster with all required add-ons:

```bash
./install.sh
```

### Verify the resources

We'll be utilizing the AWS region to run and initialize data in the upcoming steps. Let's start by setting an environment variable to the same region value you selected in previous step, by replacing the placeholder `<aws-region>` text below.

```bash
export AWS_REGION=<aws-region> 
```

Check that your EKS cluster is running in the region you set earlier:

```bash
aws eks --region $AWS_REGION describe-cluster --name trainium-inferentia
```

```bash
# Creates k8s config file to authenticate with EKS
aws eks --region $AWS_REGION update-kubeconfig --name trainium-inferentia

kubectl get nodes # Output shows the EKS Managed Node group nodes
```

</CollapsibleContent>

## 2. Launch the Llama training job

Before we launch the training script, let's first deploy a utility pod. You will get on an interactive shell of this pod to monitor the progress of the fine-tuning job, access the fined tuned model weights, and view the output generated by the fine-tuned model for sample prompts.  

```bash
kubectl apply -f training-artifact-access-pod.yaml
```

Create the ConfigMap for the training script:

```bash
kubectl apply -f llama3-finetuning-script-configmap.yaml
```

In order for the training script to be able to download the Llama 3 model from HuggingFace, it needs your HuggingFace Hub access token for authentication and accessing the model. For guidance on how to create and manage your HuggingFace tokens, please visit [Hugging Face Token Management](https://huggingface.co/docs/hub/security-tokens).

We'll set your HuggingFace Hub token as an environment variable. Replace `your_huggingface_hub_access_token` with your actual HuggingFace Hub access token.

```bash
export HUGGINGFACE_HUB_ACCESS_TOKEN=$(echo -n "your_huggingface_hub_access_token" | base64)
```

Deploy the Secret and fine-tuning Job resources by running the following command, which automatically substitutes HUGGINGFACE_HUB_ACCESS_TOKEN and AWS_REGION environment variables in the yaml before applying the resources to your Kubernetes cluster.

```bash
envsubst < lora-finetune-resources.yaml | kubectl apply -f -
```

## 4. Verify fine-tuning

Check the job status:

```bash
kubectl get jobs
```

**Note:** If the container isn't scheduled, check Karpenter logs for errors. This might happen if the chosen availability zones (AZs) or subnets lack an available trn1.32xlarge EC2 instance. To fix this, update the local.azs field in the main.tf file, located at ai-on-eks/infra/base/terraform. Ensure the trainium-trn1 EC2NodeClass in the addons.tf file, also at ai-on-eks/infra/base/terraform, references the correct subnets for these AZs. Then, rerun install.sh from ai-on-eks/infra/trainium-inferentia to apply the changes via Terraform.

To monitor the log for the fine-tuning job, access the tuned model, or check the generated text-to-SQL outputs from the test run with the fine-tuned model, open a shell in the utility pod and navigate to the `/shared` folder where these can be found. The fine-tuned model will be saved in a folder named llama3_tuned_model_<timestamp> and the generated SQL queries from sample prompts can be found in a log file named llama3_finetuning_<timestamp>.out alongside the model folder.

```bash
kubectl exec -it training-artifact-access-pod -- /bin/bash

cd /shared

ls -l llama3_tuned_model* llama3_finetuning*
```

### Cleaning up

**Note:** Always run the cleanup steps to avoid extra AWS costs.

To remove the resources created by this solution, run these commands from the root of the ai-on-eks repository:

```bash
# Delete the Kubernetes resources:
cd blueprints/training/llama-lora-finetuning-trn1
envsubst < lora-finetune-resources.yaml | kubectl delete -f -
kubectl delete -f llama3-finetuning-script-configmap.yaml
kubectl delete -f training-artifact-access-pod.yaml
```

Clean up the EKS cluster and related resources:

```bash
cd ../../../infra/trainium-inferentia/terraform
./cleanup.sh
```
