# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: trtllm-disaggregated-70b
  namespace: dynamo-cloud
spec:
  services:
    Frontend:
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 60
        periodSeconds: 60
        timeoutSeconds: 30
        failureThreshold: 10
      readinessProbe:
        exec:
          command:
            - /bin/sh
            - -c
            - 'curl -s http://localhost:8000/health | jq -e ".status == \"healthy\""'
        initialDelaySeconds: 60
        periodSeconds: 60
        timeoutSeconds: 30
        failureThreshold: 10
      dynamoNamespace: trtllm-disaggregated-70b
      componentType: main
      replicas: 1
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.4.1
          workingDir: /workspace/components/backends/trtllm
          command:
            - /bin/sh
            - -c
          args:
            - "python3 -m dynamo.frontend --http-port 8000"
    TRTLLMDecodeWorker:
      dynamoNamespace: trtllm-disaggregated-70b
      envFromSecret: hf-token-secret
      componentType: worker
      replicas: 1
      livenessProbe:
        httpGet:
          path: /live
          port: 9090
        periodSeconds: 5
        timeoutSeconds: 30
        failureThreshold: 1
      readinessProbe:
        httpGet:
          path: /health
          port: 9090
        periodSeconds: 10
        timeoutSeconds: 30
        failureThreshold: 120
      resources:
        requests:
          cpu: "16"
          memory: "64Gi"
          gpu: "4"
        limits:
          cpu: "16"
          memory: "64Gi"
          gpu: "4"
      envs:
        - name: DYN_SYSTEM_ENABLED
          value: "true"
        - name: DYN_SYSTEM_USE_ENDPOINT_HEALTH_STATUS
          value: "[\"generate\"]"
        - name: DYN_SYSTEM_PORT
          value: "9090"
      extraPodSpec:
        imagePullSecrets:
          - name: ngc-secret
        nodeSelector:
          karpenter.sh/nodepool: g6-gpu-karpenter
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 300
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.4.1
          workingDir: /workspace/components/backends/trtllm
          command:
            - /bin/sh
            - -c
          args:
            - |
              # Create inline decode configuration for 70B model
              mkdir -p /tmp/engine_configs
              cat > /tmp/engine_configs/decode.yaml << 'EOF'
              # TensorRT-LLM 70B Decode Worker Configuration
              tensor_parallel_size: 4
              moe_expert_parallel_size: 1
              enable_attention_dp: false
              max_num_tokens: 14400
              trust_remote_code: true
              backend: pytorch
              enable_chunked_prefill: true
              disable_overlap_scheduler: false
              
              cuda_graph_config:
                max_batch_size: 4
              
              kv_cache_config:
                free_gpu_memory_fraction: 0.85
              
              cache_transceiver_config:
                backend: default
              EOF
              
              # Start TensorRT-LLM decode worker for 70B model
              python3 -m dynamo.trtllm --model-path nvidia/Llama-3.3-70B-Instruct-FP8 --served-model-name nvidia/Llama-3.3-70B-Instruct-FP8 --extra-engine-args /tmp/engine_configs/decode.yaml --disaggregation-mode decode --disaggregation-strategy decode_first
    TRTLLMPrefillWorker:
      dynamoNamespace: trtllm-disaggregated-70b
      envFromSecret: hf-token-secret
      componentType: worker
      replicas: 1
      livenessProbe:
        httpGet:
          path: /live
          port: 9090
        periodSeconds: 5
        timeoutSeconds: 30
        failureThreshold: 1
      readinessProbe:
        httpGet:
          path: /health
          port: 9090
        periodSeconds: 10
        timeoutSeconds: 30
        failureThreshold: 120
      resources:
        requests:
          cpu: "16"
          memory: "64Gi"
          gpu: "4"
        limits:
          cpu: "16"
          memory: "64Gi"
          gpu: "4"
      envs:
        - name: DYN_SYSTEM_ENABLED
          value: "true"
        - name: DYN_SYSTEM_USE_ENDPOINT_HEALTH_STATUS
          value: "[\"generate\"]"
        - name: DYN_SYSTEM_PORT
          value: "9090"
      extraPodSpec:
        imagePullSecrets:
          - name: ngc-secret
        nodeSelector:
          karpenter.sh/nodepool: g6-gpu-karpenter
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 300
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.4.1
          workingDir: /workspace/components/backends/trtllm
          command:
            - /bin/sh
            - -c
          args:
            - |
              # Create inline prefill configuration for 70B model
              mkdir -p /tmp/engine_configs
              cat > /tmp/engine_configs/prefill.yaml << 'EOF'
              # TensorRT-LLM 70B Prefill Worker Configuration
              tensor_parallel_size: 4
              moe_expert_parallel_size: 1
              enable_attention_dp: false
              max_num_tokens: 14400
              trust_remote_code: true
              backend: pytorch
              enable_chunked_prefill: true
              # Overlap scheduler not currently supported in prefill only workers
              disable_overlap_scheduler: true

              cuda_graph_config:
                max_batch_size: 4

              kv_cache_config:
                free_gpu_memory_fraction: 0.85

              cache_transceiver_config:
                backend: default
              EOF

              # Start TensorRT-LLM prefill worker for 70B model
              python3 -m dynamo.trtllm --model-path nvidia/Llama-3.3-70B-Instruct-FP8 --served-model-name nvidia/Llama-3.3-70B-Instruct-FP8 --extra-engine-args /tmp/engine_configs/prefill.yaml --disaggregation-mode prefill --disaggregation-strategy decode_first
