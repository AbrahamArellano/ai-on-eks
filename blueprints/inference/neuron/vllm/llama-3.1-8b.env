# Kubernetes
SERVICE_NAME=llama-31-8b-vllm-nrn # must start with lowercase, contain only alphanumeric or dashes and end in with alphanumeric
SERVICE_NAMESPACE=default

# Infrastructure
VLLM_VERSION=0.9.1
# 1 Neuron device
NUM_GPUS=1

# Model Specific
MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
GPU_MEMORY_UTILIZATION="0.8"
MAX_MODEL_LEN=1024
MAX_NUM_SEQ=1
MAX_NUM_BATCHED_TOKENS=1024
TOKENIZER_POOL_SIZE=4
MAX_PARALLEL_LOADING_WORKERS=2
PIPELINE_PARALLEL_SIZE=1
# Neuron has 2 cores per device
TENSOR_PARALLEL_SIZE=2
ENABLE_PREFIX_CACHING=false
