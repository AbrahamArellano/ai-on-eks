{{- if eq .Values.inference.framework "lwsVllm" }}
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: {{.Values.inference.serviceName}}
  namespace: {{.Values.inference.serviceNamespace}}
spec:
  replicas: {{.Values.inference.modelServer.deployment.replicas}}
  leaderWorkerTemplate:
    size: {{.Values.modelParameters.pipelineParallelSize}}
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
          - name: vllm-leader
            image: {{.Values.inference.modelServer.image.repository}}:{{.Values.inference.modelServer.image.tag}}
            env:
              {{- if .Values.modelParameters }}
              {{- range $key, $value := .Values.modelParameters }}
              - name: {{ $key | snakecase | upper }}
                value: "{{ $value }}"
              {{- end }}
              {{- end }}
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token
                    key: token
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);
                python3 -m vllm.entrypoints.openai.api_server --port 8080 --model $MODEL_ID --tensor-parallel-size $TENSOR_PARALLEL_SIZE --pipeline_parallel_size $PIPELINE_PARALLEL_SIZE --max-model-len $MAX_MODEL_LEN"
            resources:
              limits:
                nvidia.com/gpu: "{{.Values.modelParameters.numGpus}}"
              requests:
                nvidia.com/gpu: "{{.Values.modelParameters.numGpus}}"
            ports:
              - containerPort: 8080
            readinessProbe:
              tcpSocket:
                port: 8080
              initialDelaySeconds: 60
              periodSeconds: 10
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 15Gi
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            image: {{.Values.inference.modelServer.image.repository}}:{{.Values.inference.modelServer.image.tag}}
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)"
            resources:
              limits:
                nvidia.com/gpu: "{{.Values.modelParameters.numGpus}}"
              requests:
                nvidia.com/gpu: "{{.Values.modelParameters.numGpus}}"
            env:
              {{- if .Values.modelParameters }}
              {{- range $key, $value := .Values.modelParameters }}
              - name: {{ $key | snakecase | upper }}
                value: "{{ $value }}"
              {{- end }}
              {{- end }}
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token
                    key: token
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 15Gi
---
apiVersion: v1
kind: Service
metadata:
  name: {{.Values.inference.serviceName}}-leader
  namespace: {{.Values.inference.serviceNamespace}}
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    leaderworkerset.sigs.k8s.io/name: {{.Values.inference.serviceName}}
    role: leader
  type: ClusterIP
{{- end }}
