# Kubernetes
SERVICE_NAME=llama-32-1b-vllm # must start with lowercase, contain only alphanumeric or dashes and end in with alphanumeric
SERVICE_NAMESPACE=default

# Infrastructure
VLLM_VERSION=0.9.1
NUM_GPUS=1

# Model Specific
MODEL_ID=NousResearch/Llama-3.2-1B
GPU_MEMORY_UTILIZATION="0.8"
MAX_MODEL_LEN=8192
MAX_NUM_SEQ=4
MAX_NUM_BATCHED_TOKENS=8192
TOKENIZER_POOL_SIZE=4
MAX_PARALLEL_LOADING_WORKERS=2
PIPELINE_PARALLEL_SIZE=1
TENSOR_PARALLEL_SIZE=1
ENABLE_PREFIX_CACHING=false
