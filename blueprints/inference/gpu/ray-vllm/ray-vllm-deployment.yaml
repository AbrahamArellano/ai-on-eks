apiVersion: ray.io/v1
kind: RayService
metadata:
  name: $SERVICE_NAME
  namespace: $SERVICE_NAMESPACE
spec:
  deploymentUnhealthySecondThreshold: 900
  rayClusterConfig:
    headGroupSpec:
      headService:
        metadata:
          name: $SERVICE_NAME
          namespace: $SERVICE_NAMESPACE
      rayStartParams:
        dashboard-host: 0.0.0.0
        num-cpus: '0'
      template:
        spec:
          containers:
            - env:
                - name: VLLM_LOGGING_LEVEL
                  value: DEBUG
                - name: HUGGING_FACE_HUB_TOKEN
                  valueFrom:
                    secretKeyRef:
                      key: token
                      name: hf-token
                - name: LD_LIBRARY_PATH
                  value: /home/ray/anaconda3/lib
                - name: RAY_GRAFANA_HOST
                  value: >-
                    http://kube-prometheus-stack-grafana.monitoring.svc.cluster.local
                - name: RAY_PROMETHEUS_HOST
                  value: >-
                    http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
                - name: RAY_GRAFANA_IFRAME_HOST
                  value: http://localhost:3000
              image: rayproject/ray:$RAY_VERSION-py$PYTHON_VERSION
              imagePullPolicy: Always
              lifecycle:
                preStop:
                  exec:
                    command:
                      - /bin/sh
                      - '-c'
                      - ray stop
              name: head
              ports:
                - containerPort: 6379
                  name: gcs
                  protocol: TCP
                - containerPort: 8265
                  name: dashboard
                  protocol: TCP
                - containerPort: 10001
                  name: client
                  protocol: TCP
                - containerPort: 8000
                  name: serve
                  protocol: TCP
              resources:
                limits:
                  cpu: 4
                  memory: 20Gi
                requests:
                  cpu: 4
                  memory: 20Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /home/ray/vllm_serve.py
                  subPath: vllm_serve.py
                  name: vllm-script
            - name: fluentbit
              image: fluent/fluent-bit:3.2.2
              # Get Kubernetes metadata via downward API
              env:
                - name: POD_LABELS
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.labels['ray.io/cluster']
              # These resource requests for Fluent Bit should be sufficient in production.
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 100m
                  memory: 128Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /fluent-bit/etc/fluent-bit.conf
                  subPath: fluent-bit.conf
                  name: fluentbit-config
          volumes:
            - emptyDir: {}
              name: ray-logs
            - configMap:
                items:
                  - key: vllm_serve.py
                    path: vllm_serve.py
                name: ray-vllm-serve
              name: vllm-script
            - name: fluentbit-config
              configMap:
                name: fluentbit-config
    rayVersion: "$RAY_VERSION"
    workerGroupSpecs:
      - groupName: worker
        maxReplicas: $MAX_REPLICAS
        minReplicas: $MIN_REPLICAS
        numOfHosts: 1
        rayStartParams: {}
        replicas: 1
        template:
          spec:
            containers:
              - env:
                  - name: LD_LIBRARY_PATH
                    value: /home/ray/anaconda3/lib
                  - name: VLLM_PORT
                    value: '8004'
                  - name: VLLM_LOGGING_LEVEL
                    value: DEBUG
                  - name: HUGGING_FACE_HUB_TOKEN
                    valueFrom:
                      secretKeyRef:
                        key: token
                        name: hf-token
                  - name: MODEL_ID
                    value: "$MODEL_ID"
                  - name: GPU_MEMORY_UTILIZATION
                    value: "$GPU_MEMORY_UTILIZATION"
                  - name: MAX_MODEL_LEN
                    value: "$MAX_MODEL_LEN"
                  - name: MAX_NUM_SEQ
                    value: "$MAX_NUM_SEQ"
                  - name: MAX_NUM_BATCHED_TOKENS
                    value: "$MAX_NUM_BATCHED_TOKENS"
                  - name: TOKENIZER_POOL_SIZE
                    value: "$TOKENIZER_POOL_SIZE"
                  - name: MAX_PARALLEL_LOADING_WORKERS
                    value: "$MAX_PARALLEL_LOADING_WORKERS"
                  - name: PIPELINE_PARALLEL_SIZE
                    value: "$PIPELINE_PARALLEL_SIZE"
                  - name: TENSOR_PARALLEL_SIZE
                    value: "$TENSOR_PARALLEL_SIZE"
                  - name: ENABLE_PREFIX_CACHING
                    value: "$ENABLE_PREFIX_CACHING"
                  - name: NUM_GPUS
                    value: "$NUM_GPUS"
                image: rayproject/ray:$RAY_VERSION-py$PYTHON_VERSION-gpu
                imagePullPolicy: IfNotPresent
                lifecycle:
                  preStop:
                    exec:
                      command:
                        - /bin/sh
                        - '-c'
                        - ray stop
                name: worker
                resources:
                  limits:
                    nvidia.com/gpu: $NUM_GPUS
                  requests:
                    nvidia.com/gpu: $NUM_GPUS
                volumeMounts:
                  - mountPath: /tmp/ray
                    name: ray-logs
              - name: fluentbit
                image: fluent/fluent-bit:3.2.2
                env:
                  - name: POD_LABELS
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.labels['ray.io/cluster']
                resources:
                  requests:
                    cpu: 100m
                    memory: 128Mi
                  limits:
                    cpu: 100m
                    memory: 128Mi
                volumeMounts:
                  - mountPath: /tmp/ray
                    name: ray-logs
                  - mountPath: /fluent-bit/etc/fluent-bit.conf
                    subPath: fluent-bit.conf
                    name: fluentbit-config
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            volumes:
              - emptyDir: {}
                name: ray-logs
              - name: fluentbit-config
                configMap:
                  name: fluentbit-config
  serveConfigV2: |
    applications:
      - name: serve
        import_path: vllm_serve:deployment
        runtime_env:
          pip:
            - vllm==$VLLM_VERSION
        deployments:
          - name: serve
            ray_actor_options:
              num_gpus: $NUM_GPUS
  serviceUnhealthySecondThreshold: 900
