# Kubernetes
SERVICE_NAME=llama-32-1b-ray # must start with lowercase, contain only alphanumeric or dashes and end in with alphanumeric
SERVICE_NAMESPACE=default

# Infrastructure
RAY_VERSION=2.47.0
VLLM_VERSION=0.9.1
PYTHON_VERSION=3.11
NUM_GPUS=1

# Node Scaling Configuration
MIN_REPLICAS=1
MAX_REPLICAS=2

# Model Specific
MODEL_ID=NousResearch/Llama-3.2-1B
GPU_MEMORY_UTILIZATION="0.8"
MAX_MODEL_LEN=8192
MAX_NUM_SEQ=4
MAX_NUM_BATCHED_TOKENS=8192
TOKENIZER_POOL_SIZE=4
MAX_PARALLEL_LOADING_WORKERS=2
PIPELINE_PARALLEL_SIZE=1
TENSOR_PARALLEL_SIZE=1
ENABLE_PREFIX_CACHING=true
