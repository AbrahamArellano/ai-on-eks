apiVersion: v1
kind: ConfigMap
metadata:
  name: llama3-finetuning-script
data:
  llama3_finetuning.sh: |-
    #!/bin/bash

    # Generate timestamp and set the model output directory
    TIMESTAMP=$(date +%Y%m%d-%H%M%S)
    MODEL_OUTPUT_DIR="/shared/llama3_tuned_model_${TIMESTAMP}"

    # Download the three required files directly from GitHub
    curl -O https://raw.githubusercontent.com/ai-on-eks/main/blueprints/training/llama-lora-finetuning-trn1/assets/finetune_llama.py
    curl -O https://raw.githubusercontent.com/ai-on-eks/main/blueprints/training/llama-lora-finetuning-trn1/assets/consolidate_adapter_shards_and_merge_model.py
    curl -O https://raw.githubusercontent.com/ai-on-eks/main/blueprints/training/llama-lora-finetuning-trn1/assets/test_model.py

    export FINETUNED_SHARDED_MODEL_DIR="./finetuned_sharded_model"
    export FINETUNED_MERGED_MODEL_DIR="./finetuned_merged_model"

    torchrun \
        --nproc_per_node=32 \
        finetune_llama.py \
        --output_dir $FINETUNED_SHARDED_MODEL_DIR \
        --tensor_parallel_size 8 \
        --bf16 True \
        --per_device_train_batch_size 8 \
        --gradient_accumulation_steps 1 \
        --gradient_checkpointing True \
        --max_steps 250 \
        --logging_steps 10 \
        --learning_rate 1e-5 \
        --dataloader_drop_last True

    echo -e "\n\nModel fine-tuning completed. Fine-tuned sharded model stored at: $FINETUNED_SHARDED_MODEL_DIR\n"

    echo -e "\n\nConsolidating the shards of the fine-tuned model\n"
    python3 ./consolidate_adapter_shards_and_merge_model.py -i $FINETUNED_SHARDED_MODEL_DIR/checkpoint-250 -o $FINETUNED_MERGED_MODEL_DIR
    echo -e "\n\nFine-tuned merged model stored at: $FINETUNED_MERGED_MODEL_DIR\n"

    echo -e "\n\nStarting a test run with the fine-tuned model\n"
    python3 ./test_model.py --tuned-model $FINETUNED_MERGED_MODEL_DIR
    echo -e "\n\nTest run completed"

    echo -e "\n\n saving fine-tuned model to output folder: $MODEL_OUTPUT_DIR"
    mv $FINETUNED_MERGED_MODEL_DIR $MODEL_OUTPUT_DIR
