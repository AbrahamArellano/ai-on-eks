---
# Pod 1: Large training workload (3g.40gb)
apiVersion: v1
kind: Pod
metadata:
  name: mig-large-training-pod
  namespace: mig-gpu
  labels:
    app: mig-large-training
    workload-type: training
spec:
  restartPolicy: Never
  containers:
  - name: large-training-container
    image: nvcr.io/nvidia/pytorch:25.04-py3
    command: ["python", "-c"]
    args:
    - |
      import torch
      import torch.nn as nn
      import torch.optim as optim
      import time
      import os

      print(f"=== LARGE TRAINING POD (3g.40gb) ===")
      print(f"Process ID: {os.getpid()}")
      print(f"GPU available: {torch.cuda.is_available()}")
      print(f"GPU count: {torch.cuda.device_count()}")

      if torch.cuda.is_available():
          device = torch.cuda.current_device()
          print(f"Using GPU: {torch.cuda.get_device_name(device)}")
          print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")

          # Large model for 3g.40gb instance
          model = nn.Sequential(
              nn.Linear(2048, 1024),
              nn.ReLU(),
              nn.Linear(1024, 512),
              nn.ReLU(),
              nn.Linear(512, 256),
              nn.ReLU(),
              nn.Linear(256, 10)
          ).cuda()

          optimizer = optim.Adam(model.parameters())
          criterion = nn.CrossEntropyLoss()

          print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

          # Training loop
          for epoch in range(100):
              # Large batch for 3g.40gb
              x = torch.randn(256, 2048).cuda()
              y = torch.randint(0, 10, (256,)).cuda()

              optimizer.zero_grad()
              output = model(x)
              loss = criterion(output, y)
              loss.backward()
              optimizer.step()

              if epoch % 10 == 0:
                  print(f"Large Training - Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
              time.sleep(3)

          print("Large training completed on 3g.40gb MIG instance")
    resources:
      claims:
      - name: mig-large-claim
  resourceClaims:
  - name: mig-large-claim
    resourceClaimTemplateName: mig-large-template
  nodeSelector:
    node.kubernetes.io/instance-type: p4de.24xlarge
    nvidia.com/gpu.present: "true"
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

---
# Pod 2: Medium training workload (2g.20gb) - can run on SAME GPU as Pod 1
apiVersion: v1
kind: Pod
metadata:
  name: mig-medium-training-pod
  namespace: mig-gpu
  labels:
    app: mig-medium-training
    workload-type: training
spec:
  restartPolicy: Never
  containers:
  - name: medium-training-container
    image: nvcr.io/nvidia/pytorch:25.04-py3
    command: ["python", "-c"]
    args:
    - |
      import torch
      import torch.nn as nn
      import torch.optim as optim
      import time
      import os

      print(f"=== MEDIUM TRAINING POD (2g.20gb) ===")
      print(f"Process ID: {os.getpid()}")
      print(f"GPU available: {torch.cuda.is_available()}")
      print(f"GPU count: {torch.cuda.device_count()}")

      if torch.cuda.is_available():
          device = torch.cuda.current_device()
          print(f"Using GPU: {torch.cuda.get_device_name(device)}")
          print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")

          # Medium model for 2g.20gb instance
          model = nn.Sequential(
              nn.Linear(1024, 512),
              nn.ReLU(),
              nn.Linear(512, 256),
              nn.ReLU(),
              nn.Linear(256, 10)
          ).cuda()

          optimizer = optim.Adam(model.parameters())
          criterion = nn.CrossEntropyLoss()

          print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

          # Training loop
          for epoch in range(100):
              # Medium batch for 2g.20gb
              x = torch.randn(128, 1024).cuda()
              y = torch.randint(0, 10, (128,)).cuda()

              optimizer.zero_grad()
              output = model(x)
              loss = criterion(output, y)
              loss.backward()
              optimizer.step()

              if epoch % 10 == 0:
                  print(f"Medium Training - Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
              time.sleep(4)

          print("Medium training completed on 2g.20gb MIG instance")
    resources:
      claims:
      - name: mig-medium-claim
  resourceClaims:
  - name: mig-medium-claim
    resourceClaimTemplateName: mig-medium-template
  nodeSelector:
    node.kubernetes.io/instance-type: p4de.24xlarge
    nvidia.com/gpu.present: "true"
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

---
# Pod 3: Small inference workload (1g.10gb) - can run on SAME GPU as Pod 1 & 2
apiVersion: v1
kind: Pod
metadata:
  name: mig-small-inference-pod
  namespace: mig-gpu
  labels:
    app: mig-small-inference
    workload-type: inference
spec:
  restartPolicy: Never
  containers:
  - name: small-inference-container
    image: nvcr.io/nvidia/pytorch:25.04-py3
    command: ["python", "-c"]
    args:
    - |
      import torch
      import torch.nn as nn
      import time
      import os

      print(f"=== SMALL INFERENCE POD (1g.10gb) ===")
      print(f"Process ID: {os.getpid()}")
      print(f"GPU available: {torch.cuda.is_available()}")
      print(f"GPU count: {torch.cuda.device_count()}")

      if torch.cuda.is_available():
          device = torch.cuda.current_device()
          print(f"Using GPU: {torch.cuda.get_device_name(device)}")
          print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")

          # Small model for 1g.10gb instance
          model = nn.Sequential(
              nn.Linear(512, 256),
              nn.ReLU(),
              nn.Linear(256, 10)
          ).cuda()

          print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

          # Inference loop
          for i in range(200):
              with torch.no_grad():
                  # Small batch for 1g.10gb
                  x = torch.randn(32, 512).cuda()
                  output = model(x)
                  prediction = torch.argmax(output, dim=1)

                  if i % 20 == 0:
                      print(f"Small Inference - Batch {i}, Predictions: {prediction[:5].tolist()}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
              time.sleep(2)

          print("Small inference completed on 1g.10gb MIG instance")
    resources:
      claims:
      - name: mig-small-claim
  resourceClaims:
  - name: mig-small-claim
    resourceClaimTemplateName: mig-small-template
  nodeSelector:
    node.kubernetes.io/instance-type: p4de.24xlarge
    nvidia.com/gpu.present: "true"
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
